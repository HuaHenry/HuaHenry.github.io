---
title:          "Intern-S1: A Scientific Multimodal Foundation Model"
date:           2025-08-21 00:01:00 +0800
selected:       true
# pub:            "International Conference on Machine Learning (ICML)"
# pub_pre:        "Submitted to "
pub_post:       'Technical Report'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2024"
# semantic_scholar_id: 204e3073870fae3d05bcbc2f6a8e263d9b72e776  # use this to retrieve citation count
abstract: >-
  Intern-S1 is a large multimodal MoE foundation model trained with massive scientific data and mixture-of-rewards reinforcement learning, achieving SOTA performance in scientific reasoning and professional tasks while remaining competitive in general reasoning among open-source models.
cover:          /assets/images/covers/Intern-S1.png
authors:
  - Lei Bai
  - Zhongrui Cai
  - ...
  - Zhouqi Hua
  - ...
  - Yu Qiao et al.
links:
  Paper: https://arxiv.org/abs/2508.15763
  Code: https://github.com/InternLM/Intern-S1
  Demo: https://chat.intern-ai.org.cn/
---
